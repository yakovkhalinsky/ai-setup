x-common: &common
  restart: always

x-gpu: &gpu
  runtime: nvidia
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: all
            capabilities: [gpu]

services:

  ollama:
    <<: [*gpu, *common]
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - "./ollama:/root/.ollama"

  open-webui:
    <<: [*common]
    image: ghcr.io/open-webui/open-webui:latest  
    ports:
      - "8080:8080"
    volumes:
      - ./open-webui:/app/backend/data
    extra_hosts:
      - "host.docker.internal:host-gateway"

  # searxng:
  #   <<: [*common]
  #   image: searxng/searxng:latest
  #   ports:
  #     - "8088:8080"
  #   volumes:
  #     - ./searxng/settings.yml:/etc/searxng/settings.yml

  comfyui:
    <<: [*gpu, *common]
    build:
      context: .
      dockerfile: comfyui/Dockerfile
    ports:
      - "8188:8188"
    volumes:
      - ./comfyui/models:/app/models

  # nginx:
  #   <<: [*common]
  #   image: nginx:latest
  #   ports:
  #     - "443:443"
  #   volumes:
  #     - ./ssl/rtx.pem:/etc/nginx/ssl/cert.pem
  #     - ./ssl/rtx-key.pem:/etc/nginx/ssl/key.pem
  #     - ./nginx/nginx.conf:/etc/nginx/nginx.conf

  test:
    <<: [*gpu]
    image: nvidia/cuda:12.8.1-base-ubuntu24.04
    command: nvidia-smi
